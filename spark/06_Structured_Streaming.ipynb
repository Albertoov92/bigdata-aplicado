{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19cc9931",
   "metadata": {},
   "source": [
    "### Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cca53c",
   "metadata": {},
   "source": [
    "Spark proporciona dous mecanismos para traballar con fluxos de datos (streams):\n",
    "- Spark Streaming\n",
    "- Structured Streaming\n",
    "\n",
    "**Spark Streaming**\n",
    "\n",
    "Spark Streaming é unha libraría separada en Spark para procesar fluxos de datos. Proporciona a API DStream e traballa directamente sobre RDD. (máis orientado a Batch processing)\n",
    "\n",
    "**Structured Streaming**\n",
    "\n",
    "Funciona sobre Spark SQL, polo que traballa con DataSets/Dataframes. (máis orientado a real-time processing)\n",
    "\n",
    "Nota: Todo parece indicar que Structured Streaming se convertirá no 'estándar' ao tratarse dunha ferramenta de máis alto nivel, se ben Spark Streaming é mais flexible e ao traballar directamente con RDDs permite obter mellor rendemento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8565b4f",
   "metadata": {},
   "source": [
    "#### Spark App que conta palabras a través dun fluxo de datos vía Socket TCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: aplicación que lee e conta palabras que recibe por un socket TCP.\n",
    "# https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966758c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo da estrutura da App\n",
    "# - Importamos librarías necesarias\n",
    "# - Creamos SparkSession\n",
    "# - Creamos Dataframe do Stream de entrada\n",
    "# - Lanzamos a consulta que abre a lectura do fluxo e escribe resultadoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da33e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dalgunhas funcións que usaremos para o tramento de texto\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96183ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta cela é opcional, xa que Jupyter arranca automaticamente unha SparkSession ao lanzar o notebook\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines == Dataframe que representa o fluxo de entrada de datos a partir da conexión ao socket TCP\n",
    "# Definimos o socket en localhost, porto 9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Divide as liñas en palabras\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Xenera a conta de palabras\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O DataFrame lines contén unicamente unha columna 'value' co texto de cada liña que recibe do socket\n",
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre unha terminal a lanza netcat sobre o porto 9999\n",
    "# Utilizarás a terminal como fonte de entrada de datos\n",
    "# Os datos que introduzas pola terminal serán os que lea a Spark Application\n",
    "# nc -lk 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lanza a consulta que imprime a conta de palabras por consola\n",
    "\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Antes de lanzar a query é importante abrir un socket nunha terminal co seguinte código:\n",
    "# nc -lk 9999\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc357c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parece que non hai maneira de parar o query.awaitTermination()\n",
    "# Unha solución é parar o notebook/kernel\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820d3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
