{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programa unha aplicación que vixile un directorio onde entrarán os ficheiros JSON \n",
    "# cos diferentes tweets.\n",
    "# Interésanos saber os usuarios máis activos na rede en intervalos dunha hora\n",
    "# executada cada 15 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c392dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No caso de de lanzar a aplicación desde consola debemos configurar a session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StreamingTwitterFilesWindowed\") \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb234713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a25663",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "filepath = \"file:///home/hduser/input/csv\"\n",
    "\n",
    "# A pesar de que se trata dun JSON, ao ser un streaming temos que pasarlle un Schema\n",
    "userSchema = StructType().add(\"created_at\", \"string\").add(\"screen_name\", \"string\").add(\"text\", \"string\")\n",
    "\n",
    "orixe = spark \\\n",
    "    .readStream \\\n",
    "    .schema(userSchema) \\\n",
    "    .json(filepath)\n",
    "\n",
    "# Convirto o string da data nun timestamp\n",
    "orixe_datado = orixe.withColumn('created_at',to_timestamp(orixe.created_at, 'EEE MMM d HH:mm:ss z yyyy'))\n",
    "\n",
    "procesado = orixe_datado \\\n",
    "    .groupBy(window(orixe_datado.created_at, \"60 minutes\", \"30 minutes\"),'screen_name') \\\n",
    "    .count() \\\n",
    "    .orderBy('window','count',ascending=False)\n",
    "\n",
    "\n",
    "#procesado = orixe_datado.groupBy(window(orixe_datado.created_at, \"3600 minutes\", \"1800 minutes\"),'screen_name').count().orderBy('window','count',ascending=False)\n",
    "\n",
    "consulta = procesado \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\",False) \\\n",
    "    .start()\n",
    "\n",
    "consulta.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdfbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Como exercicio extra podes repetir o exercicio mais en lugar dunha lista dos usuarios máis activos\n",
    "#  trataríase de calcular o TT ou Trending Topic, tendo en conta o número de aparicións dos HASHTAGS, \n",
    "#  palabras precedidas por un símbolo #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec72cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nunha primeira aproximación estudamos como contar os #hashtags\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "orixe = spark.read.json(\"file:///home/hduser/input/tweets/tweets.json\")\n",
    "\n",
    "#orixe.printSchema()\n",
    "\n",
    "# Usamos a mesma estratexia que no wordcount, pero filtranso despois os #hashtags\n",
    "palabras = orixe.select(explode(split(orixe.text, \" \")).alias(\"palabra\"))\n",
    "hashtags = palabras.filter(palabras.palabra.startswith('#'))\n",
    "hashtags.groupBy('palabra').count().orderBy('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5714c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora que xa sabemos que funciona, trátase de aplicar un código similar pero con streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9727551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426657a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "filepath = \"file:///home/hduser/input/csv\"\n",
    "\n",
    "# A pesar de que se trata dun JSON, ao ser un streaming temos que pasarlle un Schema\n",
    "userSchema = StructType().add(\"created_at\", \"string\").add(\"screen_name\", \"string\").add(\"text\", \"string\")\n",
    "\n",
    "orixe = spark \\\n",
    "    .readStream \\\n",
    "    .schema(userSchema) \\\n",
    "    .json(filepath)\n",
    "\n",
    "# Convirto o string da data nun timestamp\n",
    "orixe_datado = orixe.withColumn('created_at',to_timestamp(orixe.created_at, 'EEE MMM d HH:mm:ss z yyyy'))\n",
    "\n",
    "palabras = orixe_datado.select('created_at',explode(split(orixe.text, \" \")).alias(\"palabra\"))\n",
    "hashtags = palabras.filter(palabras.palabra.startswith('#'))\n",
    "\n",
    "procesado = hashtags \\\n",
    "    .groupBy(window(orixe_datado.created_at, \"60 minutes\", \"30 minutes\"),'palabra') \\\n",
    "    .count() \\\n",
    "    .orderBy('window','count',ascending=False)\n",
    "\n",
    "\n",
    "consulta = procesado \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\",False) \\\n",
    "    .start()\n",
    "\n",
    "consulta.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
