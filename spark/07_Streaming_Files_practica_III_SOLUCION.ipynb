{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb803060",
   "metadata": {},
   "source": [
    "### Structured Streaming - práctica\n",
    "\n",
    "\n",
    "#### Consultas sobre ficheiros - escritura a disco\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae un mínimo de 3 ficheiros de diferentes tamaños a partir de purchases.txt para realizar este exercicio\n",
    "# compras1.tsv <- os 5.000 primeiros rexistros\n",
    "# compras2.tsv <- 100.000 rexistros do medio do ficheiro orixinal (desde o 100.000 ao 200.000)\n",
    "# compras3.tsv <- os 20.000 últimos rexistros\n",
    "\n",
    "# Neste exercicio traballarás en local\n",
    "\n",
    "# Escolle un directorio de traballo\n",
    "# Crea un directorio input e move o fichieor compras1.tsv dentro\n",
    "# Crea un directorio store e garda dentro os ficheiros csv restantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neste exercicio a saída debe ser a disco\n",
    "# Structured Streaming proporciona diferentes tipos de \"output sinks\":\n",
    "# - consola\n",
    "# - ficheiros: csv, json, orc, parquet  (file sink)\n",
    "# - kafka\n",
    "# - ...\n",
    "\n",
    "# Escribe o resultado a 'output/resultado.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Sink:\n",
    "# outputMode : soporta unicamente \"append\n",
    "# format: csv/parquet/json/...\n",
    "# options:\n",
    "## path\n",
    "## header\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c09436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesante artigo sobre Output Sinks en medium.com\n",
    "# https://medium.com/expedia-group-tech/apache-spark-structured-streaming-output-sinks-3-of-6-ed3247545fbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da358e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lanza unha consulta filtre e garde a ficheiro unicamente as vendas cun valor > 300\n",
    "#    a partir dos ficheiros en 'input'\n",
    "\n",
    "# No primeiro momento estará compras1.txt -> espera a que se execute a consulta\n",
    "# Move o segundo ficheiro e observa o resultado\n",
    "# Move o terceiro documento e observa o resultado\n",
    "\n",
    "# Compara a evolución dos datos ao ir procesando os diferentes ficheiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e33b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d035db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa as librarías necesarias\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o esquema do Dataframe\n",
    "userSchema = StructType() \\\n",
    "    .add(\"data\", \"date\") \\\n",
    "    .add(\"hora\", \"string\") \\\n",
    "    .add(\"tenda\", \"string\") \\\n",
    "    .add(\"categoria\", \"string\") \\\n",
    "    .add(\"venda\", \"float\") \\\n",
    "    .add(\"pago\", \"string\")\n",
    "\n",
    "#userSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b917e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de17ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Dataframe para ler o fluxo de ficheiros\n",
    "tsvFiles = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .schema(userSchema) \\\n",
    "    .csv(\"file:///home/hduser/code/spark/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ea103",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvFiles.createOrReplaceTempView('compras')\n",
    "df_sql = spark.sql('SELECT * FROM compras WHERE venda > 300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a041524",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_sql \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option('header',True) \\\n",
    "    .option('path',\"file:///home/hduser/code/spark/output/resultado.csv\") \\\n",
    "    .option(\"checkpointLocation\", \"file:///home/hduser/code/spark/output/checkpoint/filesink_checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec374c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc172957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
